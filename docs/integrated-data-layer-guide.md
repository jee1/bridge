# Bridge í†µí•© ë°ì´í„° ë¶„ì„ ë ˆì´ì–´ ê°€ì´ë“œ

## ğŸš€ ê°œìš”

Bridge í†µí•© ë°ì´í„° ë¶„ì„ ë ˆì´ì–´ëŠ” **CA ë§ˆì¼ìŠ¤í†¤ 3.1**ë¡œ êµ¬í˜„ëœ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ, ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  í‘œì¤€í™”ëœ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“Š ì£¼ìš” êµ¬ì„± ìš”ì†Œ

### 1. DataUnifier - ë°ì´í„° í†µí•©
ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ í‘œì¤€ í…Œì´ë¸” í˜•íƒœë¡œ í†µí•©í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.

```python
from bridge.analytics.core import DataUnifier

# ë°ì´í„° í†µí•©ê¸° ì´ˆê¸°í™”
unifier = DataUnifier()

# ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° í†µí•©
data_sources = {
    "postgres": postgres_data,
    "mongodb": mongo_data,
    "elasticsearch": es_data
}

unified_data = unifier.unify_data_sources(
    data_sources=data_sources,
    schema_mapping=schema_mapping,
    merge_strategy="union"
)
```

### 2. SchemaMapper - ìŠ¤í‚¤ë§ˆ ë§¤í•‘
ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì˜ ìŠ¤í‚¤ë§ˆë¥¼ í‘œì¤€í™”í•˜ê³  ë§¤í•‘í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
from bridge.analytics.core import SchemaMapper, SchemaMapping, ColumnMapping

# ìŠ¤í‚¤ë§ˆ ë§¤í¼ ì´ˆê¸°í™”
mapper = SchemaMapper()

# ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ì˜
schema_mapping = SchemaMapping(
    source_name="postgres",
    target_schema={
        "user_id": ColumnMapping(
            source_column="id",
            target_column="user_id",
            data_type="integer",
            transformation="identity"
        ),
        "email": ColumnMapping(
            source_column="email_address",
            target_column="email",
            data_type="string",
            transformation="lowercase"
        )
    }
)

# ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì ìš©
mapped_data = mapper.apply_schema_mapping(data, schema_mapping)
```

### 3. TypeConverter - íƒ€ì… ë³€í™˜
ê³ ê¸‰ ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

```python
from bridge.analytics.core import TypeConverter, ConversionRule

# íƒ€ì… ë³€í™˜ê¸° ì´ˆê¸°í™”
converter = TypeConverter()

# ë³€í™˜ ê·œì¹™ ì •ì˜
conversion_rules = [
    ConversionRule(
        source_type="string",
        target_type="datetime",
        format_pattern="%Y-%m-%d",
        error_handling="coerce"
    ),
    ConversionRule(
        source_type="integer",
        target_type="float",
        transformation="divide_by_100"
    )
]

# ë°ì´í„° íƒ€ì… ë³€í™˜
converted_data = converter.convert_types(data, conversion_rules)
```

### 4. StreamingProcessor - ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œì…ë‹ˆë‹¤.

```python
from bridge.analytics.core import StreamingProcessor

# ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
processor = StreamingProcessor(
    chunk_size=10000,
    memory_limit_mb=1000
)

# ëŒ€ìš©ëŸ‰ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
def process_chunk(chunk):
    # ì²­í¬ë³„ ì²˜ë¦¬ ë¡œì§
    return chunk.apply_transformation()

processed_data = processor.process_streaming(
    data_stream=data_stream,
    processor_func=process_chunk
)
```

### 5. IntegratedDataLayer - í†µí•© ë ˆì´ì–´
ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ í†µí•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from bridge.analytics.core import IntegratedDataLayer

# í†µí•© ë°ì´í„° ë ˆì´ì–´ ì´ˆê¸°í™”
layer = IntegratedDataLayer(
    chunk_size=10000,
    memory_limit_mb=1000,
    auto_schema_mapping=True
)

# ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° í†µí•©
unified_data = layer.integrate_data_sources(
    data_sources=data_sources,
    schema_mapping=schema_mapping,
    merge_strategy="union",
    enable_streaming=True
)
```

## ğŸ”§ MCP ë„êµ¬ ì‚¬ìš©ë²•

### data_unifier ë„êµ¬
```json
{
  "tool": "data_unifier",
  "arguments": {
    "action": "unify",
    "data_sources": {
      "source1": {"data": [...]},
      "source2": {"data": [...]}
    },
    "merge_strategy": "union"
  }
}
```

### schema_mapper ë„êµ¬
```json
{
  "tool": "schema_mapper",
  "arguments": {
    "action": "map_schema",
    "data": {...},
    "mapping_rules": {
      "source_column": "target_column"
    }
  }
}
```

### type_converter ë„êµ¬
```json
{
  "tool": "type_converter",
  "arguments": {
    "action": "convert",
    "data": {...},
    "conversion_rules": [
      {
        "source_type": "string",
        "target_type": "datetime",
        "format": "%Y-%m-%d"
      }
    ]
  }
}
```

### streaming_processor ë„êµ¬
```json
{
  "tool": "streaming_processor",
  "arguments": {
    "action": "process",
    "data": {...},
    "processor_func": "transform_data",
    "chunk_size": 10000,
    "memory_limit_mb": 1000
  }
}
```

### integrated_data_layer ë„êµ¬
```json
{
  "tool": "integrated_data_layer",
  "arguments": {
    "action": "integrate",
    "data_sources": {...},
    "transformations": [...],
    "export_format": "arrow"
  }
}
```

## ğŸ“‹ ë°ì´í„° í†µí•© ì „ëµ

### Union ì „ëµ
ëª¨ë“  ì†ŒìŠ¤ì˜ ì»¬ëŸ¼ì„ í¬í•¨í•˜ë©°, ëˆ„ë½ëœ ê°’ì€ NULLë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
unified_data = layer.integrate_data_sources(
    data_sources=data_sources,
    merge_strategy="union"
)
```

### Intersection ì „ëµ
ê³µí†µ ì»¬ëŸ¼ë§Œ í¬í•¨í•˜ì—¬ ë°ì´í„°ë¥¼ í†µí•©í•©ë‹ˆë‹¤.

```python
unified_data = layer.integrate_data_sources(
    data_sources=data_sources,
    merge_strategy="intersection"
)
```

### Custom ì „ëµ
ì‚¬ìš©ì ì •ì˜ í†µí•© ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
def custom_merge_logic(source1, source2):
    # ì»¤ìŠ¤í…€ í†µí•© ë¡œì§
    return merged_data

unified_data = layer.integrate_data_sources(
    data_sources=data_sources,
    merge_strategy="custom",
    custom_merge_func=custom_merge_logic
)
```

## âš¡ ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
```python
# ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í™œì„±í™”
layer = IntegratedDataLayer(
    chunk_size=5000,  # ì‘ì€ ì²­í¬ í¬ê¸°
    memory_limit_mb=500,  # ë‚®ì€ ë©”ëª¨ë¦¬ ì œí•œ
    auto_schema_mapping=True
)

# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
unified_data = layer.integrate_data_sources(
    data_sources=large_data_sources,
    enable_streaming=True
)
```

### ë³‘ë ¬ ì²˜ë¦¬
```python
# ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬
from multiprocessing import Pool

def process_source(source_data):
    return layer.process_single_source(source_data)

with Pool(processes=4) as pool:
    results = pool.map(process_source, data_sources.values())
```

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬

### ë°ì´í„° í†µí•© ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    unified_data = layer.integrate_data_sources(data_sources)
except DataIntegrationError as e:
    logger.error(f"ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
    # ë¶€ë¶„ í†µí•© ì‹œë„
    unified_data = layer.integrate_data_sources_partial(data_sources)
```

### ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    mapped_data = mapper.apply_schema_mapping(data, schema_mapping)
except SchemaMappingError as e:
    logger.error(f"ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì‹¤íŒ¨: {e}")
    # ìë™ ë§¤í•‘ìœ¼ë¡œ ëŒ€ì²´
    auto_mapping = mapper.create_auto_mapping(data)
    mapped_data = mapper.apply_schema_mapping(data, auto_mapping)
```

## ğŸ“Š ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### 1. ë‹¤ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
```python
# PostgreSQL, MongoDB, Elasticsearch ë°ì´í„° í†µí•©
data_sources = {
    "users": postgres_users_data,
    "orders": postgres_orders_data,
    "products": mongo_products_data,
    "logs": es_logs_data
}

# ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ì˜
schema_mapping = {
    "users": {
        "id": "user_id",
        "email": "email_address",
        "created_at": "registration_date"
    },
    "orders": {
        "user_id": "customer_id",
        "order_date": "purchase_date",
        "total": "order_amount"
    }
}

# ë°ì´í„° í†µí•©
unified_data = layer.integrate_data_sources(
    data_sources=data_sources,
    schema_mapping=schema_mapping,
    merge_strategy="union"
)

# í†µí•©ëœ ë°ì´í„° ë¶„ì„
summary = layer.get_data_summary()
print(f"í†µí•©ëœ ë°ì´í„°: {summary['total_rows']}í–‰, {summary['total_columns']}ì—´")
```

### 2. ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
```python
# ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
def process_realtime_data(data_chunk):
    # ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ë¡œì§
    processed_chunk = data_chunk.filter(lambda x: x['status'] == 'active')
    return processed_chunk

# ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
streaming_processor = StreamingProcessor(chunk_size=1000)
processed_stream = streaming_processor.process_streaming(
    data_stream=realtime_data_stream,
    processor_func=process_realtime_data
)
```

### 3. ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
```python
# í†µí•©ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
from bridge.analytics.core import QualityChecker

quality_checker = QualityChecker()
quality_report = quality_checker.check_data_quality(unified_data)

# í’ˆì§ˆ ë¦¬í¬íŠ¸ ì¶œë ¥
print(f"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {quality_report.overall_score}")
print(f"ê²°ì¸¡ê°’ ë¹„ìœ¨: {quality_report.missing_value_ratio}")
print(f"ì¤‘ë³µê°’ ë¹„ìœ¨: {quality_report.duplicate_ratio}")
```

## ğŸ” ì£¼ì˜ì‚¬í•­

1. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‚¬ìš©
2. **ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„±**: í†µí•© ì „ì— ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ê·œì¹™ ì •ì˜
3. **ë°ì´í„° í’ˆì§ˆ**: í†µí•© í›„ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰
4. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì²˜ë¦¬ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
5. **ì—ëŸ¬ ë³µêµ¬**: ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ì „ëµ ì¤€ë¹„
6. **ë°ì´í„° ê²€ì¦**: í†µí•©ëœ ë°ì´í„°ì˜ ì •í™•ì„± ê²€ì¦

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [ML ì‚¬ìš© ê°€ì´ë“œ](ml-user-guide.md) - ë¨¸ì‹ ëŸ¬ë‹ ê¸°ëŠ¥ ì‚¬ìš©ë²•
- [API ì°¸ì¡° ë¬¸ì„œ](api-reference.md) - REST API ì™„ì „ ì°¸ì¡°
- [ê°œë°œì ê°€ì´ë“œ](developer-guide.md) - ê°œë°œ í™˜ê²½ ì„¤ì • ë° ê¸°ì—¬ ë°©ë²•
