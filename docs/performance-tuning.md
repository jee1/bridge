# Bridge ÏÑ±Îä• ÌäúÎãù Í∞ÄÏù¥Îìú

## üìñ Í∞úÏöî

Ïù¥ Î¨∏ÏÑúÎäî Bridge ÏãúÏä§ÌÖúÏùò ÏÑ±Îä•ÏùÑ ÏµúÏ†ÅÌôîÌïòÍ∏∞ ÏúÑÌïú Ï¢ÖÌï©Ï†ÅÏù∏ Í∞ÄÏù¥ÎìúÏûÖÎãàÎã§. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏøºÎ¶¨ ÏµúÏ†ÅÌôî, Ï∫êÏã± Ï†ÑÎûµ, ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨, Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨, Í∑∏Î¶¨Í≥† Î™®ÎãàÌÑ∞ÎßÅÏùÑ ÌÜµÌïú ÏÑ±Îä• Í∞úÏÑ† Î∞©Î≤ïÏùÑ Îã§Î£πÎãàÎã§.

## üöÄ ÏÑ±Îä• ÏµúÏ†ÅÌôî Ï†ÑÎûµ

### 1. ÏÑ±Îä• Ï∏°Ï†ï Î∞è Î™®ÎãàÌÑ∞ÎßÅ

#### ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï†ïÏùò

```python
from prometheus_client import Counter, Histogram, Gauge, Summary
import time
from functools import wraps

# Î©îÌä∏Î¶≠ Ï†ïÏùò
REQUEST_COUNT = Counter('bridge_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('bridge_request_duration_seconds', 'Request duration', ['method', 'endpoint'])
ACTIVE_CONNECTIONS = Gauge('bridge_active_connections', 'Active connections')
DATABASE_QUERY_DURATION = Histogram('bridge_db_query_duration_seconds', 'Database query duration', ['query_type'])
CACHE_HIT_RATIO = Gauge('bridge_cache_hit_ratio', 'Cache hit ratio')
MEMORY_USAGE = Gauge('bridge_memory_usage_bytes', 'Memory usage in bytes')
CPU_USAGE = Gauge('bridge_cpu_usage_percent', 'CPU usage percentage')

class PerformanceMonitor:
    """ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        self.metrics = {}
        self.start_time = time.time()
    
    def track_request(self, method: str, endpoint: str, duration: float, status: str):
        """ÏöîÏ≤≠ Ï∂îÏ†Å"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
    
    def track_database_query(self, query_type: str, duration: float):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏøºÎ¶¨ Ï∂îÏ†Å"""
        DATABASE_QUERY_DURATION.labels(query_type=query_type).observe(duration)
    
    def update_system_metrics(self):
        """ÏãúÏä§ÌÖú Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏"""
        import psutil
        process = psutil.Process()
        
        MEMORY_USAGE.set(process.memory_info().rss)
        CPU_USAGE.set(process.cpu_percent())
        ACTIVE_CONNECTIONS.set(len(process.connections()))
    
    def get_performance_summary(self) -> dict:
        """ÏÑ±Îä• ÏöîÏïΩ Ï°∞Ìöå"""
        uptime = time.time() - self.start_time
        return {
            "uptime_seconds": uptime,
            "memory_usage_mb": MEMORY_USAGE._value.get() / 1024 / 1024,
            "cpu_usage_percent": CPU_USAGE._value.get(),
            "active_connections": ACTIVE_CONNECTIONS._value.get()
        }

# ÏÑ±Îä• Ï∏°Ï†ï Îç∞ÏΩîÎ†àÏù¥ÌÑ∞
def measure_performance(metric_name: str):
    """ÏÑ±Îä• Ï∏°Ï†ï Îç∞ÏΩîÎ†àÏù¥ÌÑ∞"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start_time
                # Î©îÌä∏Î¶≠ Í∏∞Î°ù
                return result
            except Exception as e:
                duration = time.time() - start_time
                # ÏóêÎü¨ Î©îÌä∏Î¶≠ Í∏∞Î°ù
                raise
        return wrapper
    return decorator
```

#### Ïã§ÏãúÍ∞Ñ ÏÑ±Îä• ÎåÄÏãúÎ≥¥Îìú

```python
from fastapi import FastAPI, WebSocket
from typing import Dict, Any
import asyncio
import json

class PerformanceDashboard:
    """ÏÑ±Îä• ÎåÄÏãúÎ≥¥Îìú"""
    
    def __init__(self):
        self.connections = set()
        self.monitor = PerformanceMonitor()
    
    async def websocket_endpoint(self, websocket: WebSocket):
        """WebSocket ÏóîÎìúÌè¨Ïù∏Ìä∏"""
        await websocket.accept()
        self.connections.add(websocket)
        
        try:
            while True:
                # ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
                performance_data = await self._collect_performance_data()
                
                # ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÍ≤å Ï†ÑÏÜ°
                await websocket.send_text(json.dumps(performance_data))
                await asyncio.sleep(1)  # 1Ï¥àÎßàÎã§ ÏóÖÎç∞Ïù¥Ìä∏
                
        except Exception as e:
            print(f"WebSocket error: {e}")
        finally:
            self.connections.discard(websocket)
    
    async def _collect_performance_data(self) -> Dict[str, Any]:
        """ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        self.monitor.update_system_metrics()
        
        return {
            "timestamp": time.time(),
            "system": {
                "memory_usage_mb": MEMORY_USAGE._value.get() / 1024 / 1024,
                "cpu_usage_percent": CPU_USAGE._value.get(),
                "active_connections": ACTIVE_CONNECTIONS._value.get()
            },
            "requests": {
                "total_requests": REQUEST_COUNT._value.sum(),
                "avg_duration": REQUEST_DURATION._value.sum() / max(REQUEST_COUNT._value.sum(), 1)
            },
            "database": {
                "avg_query_duration": DATABASE_QUERY_DURATION._value.sum() / max(DATABASE_QUERY_DURATION._value.count(), 1)
            }
        }
```

### 2. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• ÏµúÏ†ÅÌôî

#### ÏøºÎ¶¨ ÏµúÏ†ÅÌôî

```python
from sqlalchemy import text, select, func
from sqlalchemy.orm import sessionmaker
from typing import List, Dict, Any
import time

class QueryOptimizer:
    """ÏøºÎ¶¨ ÏµúÏ†ÅÌôîÍ∏∞"""
    
    def __init__(self, session_factory):
        self.session_factory = session_factory
        self.query_cache = {}
        self.query_stats = {}
    
    async def execute_optimized_query(self, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ Ïã§Ìñâ"""
        start_time = time.time()
        
        try:
            # ÏøºÎ¶¨ Î∂ÑÏÑù
            query_plan = await self._analyze_query(query)
            
            # Ïù∏Îç±Ïä§ ÌûåÌä∏ Ï∂îÍ∞Ä
            optimized_query = await self._add_index_hints(query, query_plan)
            
            # Ïã§Ìñâ Í≥ÑÌöç ÌôïÏù∏
            execution_plan = await self._get_execution_plan(optimized_query)
            
            # ÏøºÎ¶¨ Ïã§Ìñâ
            with self.session_factory() as session:
                result = session.execute(text(optimized_query), params or {})
                rows = [dict(row) for row in result]
            
            duration = time.time() - start_time
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ Í∏∞Î°ù
            self._record_query_stats(query, duration, len(rows))
            
            return rows
            
        except Exception as e:
            duration = time.time() - start_time
            self._record_query_stats(query, duration, 0, error=str(e))
            raise
    
    async def _analyze_query(self, query: str) -> Dict[str, Any]:
        """ÏøºÎ¶¨ Î∂ÑÏÑù"""
        # ÏøºÎ¶¨ Î≥µÏû°ÎèÑ Î∂ÑÏÑù
        complexity_score = self._calculate_complexity(query)
        
        # ÌÖåÏù¥Î∏î Î∞è Ïª¨Îüº Î∂ÑÏÑù
        tables = self._extract_tables(query)
        columns = self._extract_columns(query)
        
        # Ï°∞Ïù∏ Î∂ÑÏÑù
        joins = self._analyze_joins(query)
        
        return {
            "complexity_score": complexity_score,
            "tables": tables,
            "columns": columns,
            "joins": joins
        }
    
    def _calculate_complexity(self, query: str) -> int:
        """ÏøºÎ¶¨ Î≥µÏû°ÎèÑ Í≥ÑÏÇ∞"""
        complexity = 0
        
        # SELECT Ï†à Î≥µÏû°ÎèÑ
        complexity += query.upper().count('SELECT') * 1
        
        # JOIN Î≥µÏû°ÎèÑ
        complexity += query.upper().count('JOIN') * 2
        
        # ÏÑúÎ∏åÏøºÎ¶¨ Î≥µÏû°ÎèÑ
        complexity += query.upper().count('(SELECT') * 3
        
        # ÏßëÍ≥Ñ Ìï®Ïàò Î≥µÏû°ÎèÑ
        aggregate_functions = ['COUNT', 'SUM', 'AVG', 'MAX', 'MIN', 'GROUP_CONCAT']
        for func in aggregate_functions:
            complexity += query.upper().count(func) * 1
        
        return complexity
    
    async def _add_index_hints(self, query: str, query_plan: Dict[str, Any]) -> str:
        """Ïù∏Îç±Ïä§ ÌûåÌä∏ Ï∂îÍ∞Ä"""
        # ÌÖåÏù¥Î∏îÎ≥Ñ Ïù∏Îç±Ïä§ ÌûåÌä∏ Ï∂îÍ∞Ä
        for table in query_plan.get("tables", []):
            # Í∞ÄÏû• Ï†ÅÌï©Ìïú Ïù∏Îç±Ïä§ ÏÑ†ÌÉù
            best_index = await self._select_best_index(table, query_plan)
            if best_index:
                query = query.replace(f"FROM {table}", f"FROM {table} USE INDEX ({best_index})")
        
        return query
    
    async def _select_best_index(self, table: str, query_plan: Dict[str, Any]) -> str:
        """ÏµúÏ†Å Ïù∏Îç±Ïä§ ÏÑ†ÌÉù"""
        # ÌÖåÏù¥Î∏îÏùò Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå
        indexes = await self._get_table_indexes(table)
        
        # ÏøºÎ¶¨ Ï°∞Í±¥Í≥º Îß§Ïπ≠ÎêòÎäî Ïù∏Îç±Ïä§ ÏÑ†ÌÉù
        for index in indexes:
            if self._is_index_suitable(index, query_plan):
                return index["name"]
        
        return None
    
    async def _get_table_indexes(self, table: str) -> List[Dict[str, Any]]:
        """ÌÖåÏù¥Î∏î Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå
        return []
    
    def _is_index_suitable(self, index: Dict[str, Any], query_plan: Dict[str, Any]) -> bool:
        """Ïù∏Îç±Ïä§ Ï†ÅÌï©ÏÑ± Í≤ÄÏÇ¨"""
        # Ïù∏Îç±Ïä§ Ïª¨ÎüºÍ≥º ÏøºÎ¶¨ Ï°∞Í±¥ Îß§Ïπ≠
        return True
    
    async def _get_execution_plan(self, query: str) -> Dict[str, Any]:
        """Ïã§Ìñâ Í≥ÑÌöç Ï°∞Ìöå"""
        # EXPLAIN ÏøºÎ¶¨ Ïã§Ìñâ
        with self.session_factory() as session:
            result = session.execute(text(f"EXPLAIN {query}"))
            return [dict(row) for row in result]
    
    def _record_query_stats(self, query: str, duration: float, row_count: int, error: str = None):
        """ÏøºÎ¶¨ ÌÜµÍ≥Ñ Í∏∞Î°ù"""
        query_hash = hash(query)
        
        if query_hash not in self.query_stats:
            self.query_stats[query_hash] = {
                "query": query,
                "execution_count": 0,
                "total_duration": 0,
                "total_rows": 0,
                "error_count": 0
            }
        
        stats = self.query_stats[query_hash]
        stats["execution_count"] += 1
        stats["total_duration"] += duration
        stats["total_rows"] += row_count
        
        if error:
            stats["error_count"] += 1
```

#### Ïó∞Í≤∞ ÌíÄ ÏµúÏ†ÅÌôî

```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool, StaticPool
from sqlalchemy.engine import Engine
import asyncio

class DatabaseConnectionManager:
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, database_url: str, max_connections: int = 20):
        self.database_url = database_url
        self.max_connections = max_connections
        self.engine = self._create_optimized_engine()
        self.session_factory = sessionmaker(bind=self.engine)
    
    def _create_optimized_engine(self) -> Engine:
        """ÏµúÏ†ÅÌôîÎêú ÏóîÏßÑ ÏÉùÏÑ±"""
        engine = create_engine(
            self.database_url,
            poolclass=QueuePool,
            pool_size=10,  # Í∏∞Î≥∏ Ïó∞Í≤∞ Ïàò
            max_overflow=20,  # Ï∂îÍ∞Ä Ïó∞Í≤∞ Ïàò
            pool_pre_ping=True,  # Ïó∞Í≤∞ ÏÉÅÌÉú ÌôïÏù∏
            pool_recycle=3600,  # 1ÏãúÍ∞ÑÎßàÎã§ Ïó∞Í≤∞ Ïû¨ÏÉùÏÑ±
            echo=False,  # SQL Î°úÍπÖ ÎπÑÌôúÏÑ±Ìôî
            connect_args={
                "connect_timeout": 10,
                "application_name": "bridge_analytics"
            }
        )
        return engine
    
    async def get_connection(self):
        """Ïó∞Í≤∞ ÌöçÎìù"""
        return self.engine.connect()
    
    async def execute_query(self, query: str, params: Dict[str, Any] = None):
        """ÏøºÎ¶¨ Ïã§Ìñâ"""
        async with self.get_connection() as conn:
            result = await conn.execute(text(query), params or {})
            return [dict(row) for row in result]
    
    def get_pool_status(self) -> Dict[str, Any]:
        """Ïó∞Í≤∞ ÌíÄ ÏÉÅÌÉú Ï°∞Ìöå"""
        pool = self.engine.pool
        return {
            "pool_size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid()
        }
```

### 3. Ï∫êÏã± Ï†ÑÎûµ

#### Îã§Ï∏µ Ï∫êÏã± ÏãúÏä§ÌÖú

```python
import redis
import json
import pickle
from typing import Any, Optional, Union
from functools import wraps
import hashlib

class MultiLayerCache:
    """Îã§Ï∏µ Ï∫êÏã± ÏãúÏä§ÌÖú"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        self.redis_client = redis.from_url(redis_url)
        self.local_cache = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "local_hits": 0,
            "redis_hits": 0
        }
    
    def cache_key(self, func_name: str, *args, **kwargs) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        key_data = f"{func_name}:{args}:{kwargs}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """Ï∫êÏãúÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå"""
        # 1Îã®Í≥Ñ: Î°úÏª¨ Ï∫êÏãú ÌôïÏù∏
        if key in self.local_cache:
            self.cache_stats["hits"] += 1
            self.cache_stats["local_hits"] += 1
            return self.local_cache[key]
        
        # 2Îã®Í≥Ñ: Redis Ï∫êÏãú ÌôïÏù∏
        try:
            cached_data = self.redis_client.get(key)
            if cached_data:
                data = pickle.loads(cached_data)
                self.local_cache[key] = data  # Î°úÏª¨ Ï∫êÏãúÏóê Ï†ÄÏû•
                self.cache_stats["hits"] += 1
                self.cache_stats["redis_hits"] += 1
                return data
        except Exception as e:
            print(f"Redis cache error: {e}")
        
        self.cache_stats["misses"] += 1
        return None
    
    def set(self, key: str, data: Any, ttl: int = 3600):
        """Ï∫êÏãúÏóê Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
        # Î°úÏª¨ Ï∫êÏãú Ï†ÄÏû•
        self.local_cache[key] = data
        
        # Redis Ï∫êÏãú Ï†ÄÏû•
        try:
            serialized_data = pickle.dumps(data)
            self.redis_client.setex(key, ttl, serialized_data)
        except Exception as e:
            print(f"Redis cache set error: {e}")
    
    def invalidate(self, pattern: str):
        """Ìå®ÌÑ¥Ïóê ÎßûÎäî Ï∫êÏãú Î¨¥Ìö®Ìôî"""
        # Î°úÏª¨ Ï∫êÏãú Î¨¥Ìö®Ìôî
        keys_to_remove = [k for k in self.local_cache.keys() if pattern in k]
        for key in keys_to_remove:
            del self.local_cache[key]
        
        # Redis Ï∫êÏãú Î¨¥Ìö®Ìôî
        try:
            keys = self.redis_client.keys(f"*{pattern}*")
            if keys:
                self.redis_client.delete(*keys)
        except Exception as e:
            print(f"Redis cache invalidation error: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Ï∫êÏãú ÌÜµÍ≥Ñ Ï°∞Ìöå"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_ratio = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            **self.cache_stats,
            "hit_ratio": hit_ratio,
            "local_cache_size": len(self.local_cache)
        }

# Ï∫êÏã± Îç∞ÏΩîÎ†àÏù¥ÌÑ∞
def cached(ttl: int = 3600, cache_key_func: callable = None):
    """Ï∫êÏã± Îç∞ÏΩîÎ†àÏù¥ÌÑ∞"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = MultiLayerCache()
            
            # Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±
            if cache_key_func:
                key = cache_key_func(*args, **kwargs)
            else:
                key = cache.cache_key(func.__name__, *args, **kwargs)
            
            # Ï∫êÏãúÏóêÏÑú Ï°∞Ìöå
            cached_result = cache.get(key)
            if cached_result is not None:
                return cached_result
            
            # Ìï®Ïàò Ïã§Ìñâ
            result = await func(*args, **kwargs)
            
            # Í≤∞Í≥º Ï∫êÏã±
            cache.set(key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

#### ÏøºÎ¶¨ Í≤∞Í≥º Ï∫êÏã±

```python
class QueryResultCache:
    """ÏøºÎ¶¨ Í≤∞Í≥º Ï∫êÏã±"""
    
    def __init__(self, cache_manager: MultiLayerCache):
        self.cache_manager = cache_manager
        self.query_cache_ttl = {
            "metadata": 3600,  # 1ÏãúÍ∞Ñ
            "statistics": 1800,  # 30Î∂Ñ
            "quality_check": 900,  # 15Î∂Ñ
            "chart_data": 600,  # 10Î∂Ñ
        }
    
    def cache_query_result(self, query_type: str, query: str, params: Dict[str, Any], result: Any):
        """ÏøºÎ¶¨ Í≤∞Í≥º Ï∫êÏã±"""
        cache_key = self._generate_cache_key(query_type, query, params)
        ttl = self.query_cache_ttl.get(query_type, 3600)
        self.cache_manager.set(cache_key, result, ttl)
    
    def get_cached_result(self, query_type: str, query: str, params: Dict[str, Any]) -> Optional[Any]:
        """Ï∫êÏãúÎêú Í≤∞Í≥º Ï°∞Ìöå"""
        cache_key = self._generate_cache_key(query_type, query, params)
        return self.cache_manager.get(cache_key)
    
    def _generate_cache_key(self, query_type: str, query: str, params: Dict[str, Any]) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        key_data = f"{query_type}:{query}:{params}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def invalidate_query_cache(self, query_type: str = None):
        """ÏøºÎ¶¨ Ï∫êÏãú Î¨¥Ìö®Ìôî"""
        if query_type:
            self.cache_manager.invalidate(f"{query_type}:")
        else:
            self.cache_manager.invalidate("")
```

### 4. ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏµúÏ†ÅÌôî

#### ÎπÑÎèôÍ∏∞ ÏûëÏóÖ ÌÅê

```python
from celery import Celery
from celery.result import AsyncResult
from typing import Dict, Any, List
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncTaskManager:
    """ÎπÑÎèôÍ∏∞ ÏûëÏóÖ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, celery_app: Celery, max_workers: int = 4):
        self.celery_app = celery_app
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_tasks = {}
    
    async def submit_task(self, task_name: str, args: tuple = (), kwargs: dict = None) -> str:
        """ÏûëÏóÖ Ï†úÏ∂ú"""
        task = self.celery_app.send_task(task_name, args=args, kwargs=kwargs or {})
        self.active_tasks[task.id] = {
            "task": task,
            "status": "PENDING",
            "created_at": time.time()
        }
        return task.id
    
    async def get_task_result(self, task_id: str) -> Dict[str, Any]:
        """ÏûëÏóÖ Í≤∞Í≥º Ï°∞Ìöå"""
        if task_id not in self.active_tasks:
            return {"error": "Task not found"}
        
        task_info = self.active_tasks[task_id]
        task = task_info["task"]
        
        if task.ready():
            if task.successful():
                result = task.result
                task_info["status"] = "SUCCESS"
                return {
                    "status": "SUCCESS",
                    "result": result
                }
            else:
                error = str(task.result)
                task_info["status"] = "FAILURE"
                return {
                    "status": "FAILURE",
                    "error": error
                }
        else:
            return {
                "status": "PENDING",
                "progress": task_info.get("progress", 0)
            }
    
    async def execute_parallel_tasks(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Î≥ëÎ†¨ ÏûëÏóÖ Ïã§Ìñâ"""
        async def execute_single_task(task_config):
            task_name = task_config["name"]
            args = task_config.get("args", ())
            kwargs = task_config.get("kwargs", {})
            
            return await self.submit_task(task_name, args, kwargs)
        
        # Î™®Îì† ÏûëÏóÖÏùÑ Î≥ëÎ†¨Î°ú Ï†úÏ∂ú
        task_ids = await asyncio.gather(*[execute_single_task(task) for task in tasks])
        
        # Í≤∞Í≥º ÏàòÏßë
        results = []
        for task_id in task_ids:
            result = await self.get_task_result(task_id)
            results.append({"task_id": task_id, **result})
        
        return results
    
    async def batch_process(self, items: List[Any], process_func: callable, 
                          batch_size: int = 100) -> List[Any]:
        """Î∞∞Ïπò Ï≤òÎ¶¨"""
        results = []
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            
            # Î∞∞ÏπòÎ•º Î≥ëÎ†¨Î°ú Ï≤òÎ¶¨
            batch_tasks = [
                asyncio.create_task(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor, process_func, item
                    )
                ) for item in batch
            ]
            
            batch_results = await asyncio.gather(*batch_tasks)
            results.extend(batch_results)
        
        return results
```

#### Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨

```python
import pandas as pd
import pyarrow as pa
from typing import Iterator, Any
import gc

class MemoryEfficientProcessor:
    """Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨Í∏∞"""
    
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
    
    def process_large_dataset(self, data_source: str, process_func: callable) -> Iterator[Any]:
        """ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞ÏÖã Ï≤òÎ¶¨"""
        # Ï≤≠ÌÅ¨ Îã®ÏúÑÎ°ú Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞
        for chunk in self._read_data_in_chunks(data_source):
            try:
                # Ï≤≠ÌÅ¨ Ï≤òÎ¶¨
                processed_chunk = process_func(chunk)
                yield processed_chunk
                
                # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
                del chunk
                gc.collect()
                
            except Exception as e:
                print(f"Chunk processing error: {e}")
                continue
    
    def _read_data_in_chunks(self, data_source: str) -> Iterator[pd.DataFrame]:
        """Ï≤≠ÌÅ¨ Îã®ÏúÑÎ°ú Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑúÎÇò ÌååÏùº Ïä§Ìä∏Î¶º ÏÇ¨Ïö©
        offset = 0
        
        while True:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï≤≠ÌÅ¨ ÏùΩÍ∏∞
            chunk = self._read_chunk_from_db(data_source, offset, self.chunk_size)
            
            if chunk.empty:
                break
            
            yield chunk
            offset += self.chunk_size
    
    def _read_chunk_from_db(self, data_source: str, offset: int, limit: int) -> pd.DataFrame:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï≤≠ÌÅ¨ ÏùΩÍ∏∞"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏøºÎ¶¨ Ïã§Ìñâ
        return pd.DataFrame()
    
    def optimize_memory_usage(self, df: pd.DataFrame) -> pd.DataFrame:
        """Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÏµúÏ†ÅÌôî"""
        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÏµúÏ†ÅÌôî
        for col in df.columns:
            if df[col].dtype == 'object':
                # Î¨∏ÏûêÏó¥ Ïª¨Îüº ÏµúÏ†ÅÌôî
                df[col] = df[col].astype('category')
            elif df[col].dtype == 'int64':
                # Ï†ïÏàò Ïª¨Îüº ÏµúÏ†ÅÌôî
                if df[col].min() >= 0:
                    if df[col].max() < 255:
                        df[col] = df[col].astype('uint8')
                    elif df[col].max() < 65535:
                        df[col] = df[col].astype('uint16')
                    elif df[col].max() < 4294967295:
                        df[col] = df[col].astype('uint32')
                else:
                    if df[col].min() > -128 and df[col].max() < 127:
                        df[col] = df[col].astype('int8')
                    elif df[col].min() > -32768 and df[col].max() < 32767:
                        df[col] = df[col].astype('int16')
                    elif df[col].min() > -2147483648 and df[col].max() < 2147483647:
                        df[col] = df[col].astype('int32')
            elif df[col].dtype == 'float64':
                # Ïã§Ïàò Ïª¨Îüº ÏµúÏ†ÅÌôî
                df[col] = pd.to_numeric(df[col], downcast='float')
        
        return df
```

### 5. API ÏÑ±Îä• ÏµúÏ†ÅÌôî

#### ÏùëÎãµ ÏïïÏ∂ï

```python
from fastapi import FastAPI, Request, Response
from fastapi.middleware.gzip import GZipMiddleware
import gzip
import json

def setup_response_compression(app: FastAPI):
    """ÏùëÎãµ ÏïïÏ∂ï ÏÑ§Ï†ï"""
    app.add_middleware(GZipMiddleware, minimum_size=1000)
    
    @app.middleware("http")
    async def compress_response(request: Request, call_next):
        response = await call_next(request)
        
        # JSON ÏùëÎãµ ÏïïÏ∂ï
        if (response.headers.get("content-type", "").startswith("application/json") and
            len(response.body) > 1000):
            
            compressed_body = gzip.compress(response.body)
            response.body = compressed_body
            response.headers["content-encoding"] = "gzip"
            response.headers["content-length"] = str(len(compressed_body))
        
        return response
```

#### ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò ÏµúÏ†ÅÌôî

```python
from typing import List, Dict, Any, Optional
from sqlalchemy import select, func

class PaginatedQuery:
    """ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò ÏøºÎ¶¨"""
    
    def __init__(self, query, page: int = 1, page_size: int = 20):
        self.query = query
        self.page = max(1, page)
        self.page_size = min(100, max(1, page_size))  # ÏµúÎåÄ 100Í∞úÎ°ú Ï†úÌïú
        self.offset = (self.page - 1) * self.page_size
    
    async def execute(self, session) -> Dict[str, Any]:
        """ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò ÏøºÎ¶¨ Ïã§Ìñâ"""
        # Ï¥ù Í∞úÏàò Ï°∞Ìöå
        count_query = select(func.count()).select_from(self.query.subquery())
        total_count = session.execute(count_query).scalar()
        
        # ÌéòÏù¥ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
        paginated_query = self.query.offset(self.offset).limit(self.page_size)
        results = session.execute(paginated_query).fetchall()
        
        # ÌéòÏù¥ÏßÄ Ï†ïÎ≥¥ Í≥ÑÏÇ∞
        total_pages = (total_count + self.page_size - 1) // self.page_size
        has_next = self.page < total_pages
        has_prev = self.page > 1
        
        return {
            "data": [dict(row) for row in results],
            "pagination": {
                "page": self.page,
                "page_size": self.page_size,
                "total_count": total_count,
                "total_pages": total_pages,
                "has_next": has_next,
                "has_prev": has_prev
            }
        }
```

#### API ÏùëÎãµ Ï∫êÏã±

```python
from fastapi import FastAPI, Request, Response
from typing import Callable
import hashlib
import json

class APICacheMiddleware:
    """API Ï∫êÏã± ÎØ∏Îì§Ïõ®Ïñ¥"""
    
    def __init__(self, cache_manager: MultiLayerCache):
        self.cache_manager = cache_manager
        self.cacheable_endpoints = {
            "/api/v1/analytics/statistics": 1800,  # 30Î∂Ñ
            "/api/v1/connectors/metadata": 3600,   # 1ÏãúÍ∞Ñ
            "/api/v1/dashboards": 600,            # 10Î∂Ñ
        }
    
    def __call__(self, request: Request, call_next: Callable) -> Response:
        # Ï∫êÏãú Í∞ÄÎä•Ìïú ÏóîÎìúÌè¨Ïù∏Ìä∏ ÌôïÏù∏
        if request.url.path not in self.cacheable_endpoints:
            return call_next(request)
        
        # Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±
        cache_key = self._generate_cache_key(request)
        
        # Ï∫êÏãúÏóêÏÑú ÏùëÎãµ Ï°∞Ìöå
        cached_response = self.cache_manager.get(cache_key)
        if cached_response:
            return Response(
                content=cached_response["content"],
                status_code=cached_response["status_code"],
                headers=cached_response["headers"]
            )
        
        # ÏõêÎ≥∏ ÏöîÏ≤≠ Ï≤òÎ¶¨
        response = call_next(request)
        
        # ÏùëÎãµ Ï∫êÏã±
        if response.status_code == 200:
            ttl = self.cacheable_endpoints[request.url.path]
            self.cache_manager.set(cache_key, {
                "content": response.body,
                "status_code": response.status_code,
                "headers": dict(response.headers)
            }, ttl)
        
        return response
    
    def _generate_cache_key(self, request: Request) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        key_data = f"{request.url.path}:{request.query_params}:{request.headers.get('authorization', '')}"
        return hashlib.md5(key_data.encode()).hexdigest()
```

### 6. Î™®ÎãàÌÑ∞ÎßÅ Î∞è ÏïåÎ¶º

#### ÏÑ±Îä• ÏûÑÍ≥ÑÍ∞í Î™®ÎãàÌÑ∞ÎßÅ

```python
from typing import Dict, Any, List
import asyncio
import smtplib
from email.mime.text import MIMEText

class PerformanceAlertManager:
    """ÏÑ±Îä• ÏïåÎ¶º Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self):
        self.thresholds = {
            "response_time": 2.0,  # 2Ï¥à
            "memory_usage": 80.0,  # 80%
            "cpu_usage": 90.0,     # 90%
            "error_rate": 5.0,     # 5%
            "cache_hit_ratio": 0.7  # 70%
        }
        self.alert_history = []
        self.notification_channels = []
    
    def add_notification_channel(self, channel_type: str, config: Dict[str, Any]):
        """ÏïåÎ¶º Ï±ÑÎÑê Ï∂îÍ∞Ä"""
        self.notification_channels.append({
            "type": channel_type,
            "config": config
        })
    
    async def check_performance_metrics(self, metrics: Dict[str, Any]):
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ ÌôïÏù∏"""
        alerts = []
        
        # ÏùëÎãµ ÏãúÍ∞Ñ ÌôïÏù∏
        if metrics.get("avg_response_time", 0) > self.thresholds["response_time"]:
            alerts.append({
                "type": "high_response_time",
                "message": f"ÌèâÍ∑† ÏùëÎãµ ÏãúÍ∞ÑÏù¥ {metrics['avg_response_time']:.2f}Ï¥àÎ°ú ÏûÑÍ≥ÑÍ∞íÏùÑ Ï¥àÍ≥ºÌñàÏäµÎãàÎã§",
                "severity": "warning"
            })
        
        # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÌôïÏù∏
        memory_usage = metrics.get("memory_usage_percent", 0)
        if memory_usage > self.thresholds["memory_usage"]:
            alerts.append({
                "type": "high_memory_usage",
                "message": f"Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ {memory_usage:.1f}%Î°ú ÏûÑÍ≥ÑÍ∞íÏùÑ Ï¥àÍ≥ºÌñàÏäµÎãàÎã§",
                "severity": "critical" if memory_usage > 95 else "warning"
            })
        
        # CPU ÏÇ¨Ïö©Îüâ ÌôïÏù∏
        cpu_usage = metrics.get("cpu_usage_percent", 0)
        if cpu_usage > self.thresholds["cpu_usage"]:
            alerts.append({
                "type": "high_cpu_usage",
                "message": f"CPU ÏÇ¨Ïö©ÎüâÏù¥ {cpu_usage:.1f}%Î°ú ÏûÑÍ≥ÑÍ∞íÏùÑ Ï¥àÍ≥ºÌñàÏäµÎãàÎã§",
                "severity": "critical" if cpu_usage > 95 else "warning"
            })
        
        # ÏóêÎü¨Ïú® ÌôïÏù∏
        error_rate = metrics.get("error_rate", 0)
        if error_rate > self.thresholds["error_rate"]:
            alerts.append({
                "type": "high_error_rate",
                "message": f"ÏóêÎü¨Ïú®Ïù¥ {error_rate:.1f}%Î°ú ÏûÑÍ≥ÑÍ∞íÏùÑ Ï¥àÍ≥ºÌñàÏäµÎãàÎã§",
                "severity": "critical"
            })
        
        # Ï∫êÏãú ÌûàÌä∏Ïú® ÌôïÏù∏
        cache_hit_ratio = metrics.get("cache_hit_ratio", 1.0)
        if cache_hit_ratio < self.thresholds["cache_hit_ratio"]:
            alerts.append({
                "type": "low_cache_hit_ratio",
                "message": f"Ï∫êÏãú ÌûàÌä∏Ïú®Ïù¥ {cache_hit_ratio:.1%}Î°ú ÏûÑÍ≥ÑÍ∞í ÎØ∏ÎßåÏûÖÎãàÎã§",
                "severity": "warning"
            })
        
        # ÏïåÎ¶º Ï†ÑÏÜ°
        for alert in alerts:
            await self._send_alert(alert)
    
    async def _send_alert(self, alert: Dict[str, Any]):
        """ÏïåÎ¶º Ï†ÑÏÜ°"""
        for channel in self.notification_channels:
            try:
                if channel["type"] == "email":
                    await self._send_email_alert(alert, channel["config"])
                elif channel["type"] == "slack":
                    await self._send_slack_alert(alert, channel["config"])
            except Exception as e:
                print(f"Alert sending failed: {e}")
    
    async def _send_email_alert(self, alert: Dict[str, Any], config: Dict[str, Any]):
        """Ïù¥Î©îÏùº ÏïåÎ¶º Ï†ÑÏÜ°"""
        msg = MIMEText(alert["message"])
        msg["Subject"] = f"Bridge Performance Alert: {alert['type']}"
        msg["From"] = config["from_email"]
        msg["To"] = config["to_email"]
        
        # SMTP ÏÑúÎ≤ÑÎ•º ÌÜµÌïú Ïù¥Î©îÏùº Ï†ÑÏÜ°
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî ÎπÑÎèôÍ∏∞ SMTP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÇ¨Ïö©
        pass
    
    async def _send_slack_alert(self, alert: Dict[str, Any], config: Dict[str, Any]):
        """Ïä¨Îûô ÏïåÎ¶º Ï†ÑÏÜ°"""
        # Ïä¨Îûô ÏõπÌõÖÏùÑ ÌÜµÌïú ÏïåÎ¶º Ï†ÑÏÜ°
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî aiohttpÎ•º ÏÇ¨Ïö©Ìïú ÎπÑÎèôÍ∏∞ ÏöîÏ≤≠
        pass
```

## üìä ÏÑ±Îä• Î≤§ÏπòÎßàÌÇπ

### 1. Î∂ÄÌïò ÌÖåÏä§Ìä∏

```python
import asyncio
import aiohttp
import time
from typing import List, Dict, Any
import statistics

class LoadTester:
    """Î∂ÄÌïò ÌÖåÏä§Ìä∏ ÎèÑÍµ¨"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.results = []
    
    async def run_load_test(self, endpoint: str, concurrent_users: int, 
                           duration_seconds: int, request_data: Dict[str, Any] = None):
        """Î∂ÄÌïò ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(concurrent_users):
                task = asyncio.create_task(
                    self._simulate_user(session, endpoint, request_data, end_time)
                )
                tasks.append(task)
            
            await asyncio.gather(*tasks)
        
        return self._analyze_results()
    
    async def _simulate_user(self, session: aiohttp.ClientSession, endpoint: str, 
                           request_data: Dict[str, Any], end_time: float):
        """ÏÇ¨Ïö©Ïûê ÏãúÎÆ¨Î†àÏù¥ÏÖò"""
        while time.time() < end_time:
            start_time = time.time()
            
            try:
                if request_data:
                    async with session.post(f"{self.base_url}{endpoint}", 
                                          json=request_data) as response:
                        await response.text()
                else:
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        await response.text()
                
                duration = time.time() - start_time
                self.results.append({
                    "timestamp": start_time,
                    "duration": duration,
                    "status_code": response.status,
                    "success": response.status < 400
                })
                
            except Exception as e:
                duration = time.time() - start_time
                self.results.append({
                    "timestamp": start_time,
                    "duration": duration,
                    "status_code": 0,
                    "success": False,
                    "error": str(e)
                })
            
            # ÏöîÏ≤≠ Í∞Ñ Í∞ÑÍ≤©
            await asyncio.sleep(0.1)
    
    def _analyze_results(self) -> Dict[str, Any]:
        """Í≤∞Í≥º Î∂ÑÏÑù"""
        if not self.results:
            return {}
        
        successful_requests = [r for r in self.results if r["success"]]
        failed_requests = [r for r in self.results if not r["success"]]
        
        durations = [r["duration"] for r in successful_requests]
        
        return {
            "total_requests": len(self.results),
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "success_rate": len(successful_requests) / len(self.results) if self.results else 0,
            "avg_response_time": statistics.mean(durations) if durations else 0,
            "min_response_time": min(durations) if durations else 0,
            "max_response_time": max(durations) if durations else 0,
            "p95_response_time": self._percentile(durations, 95) if durations else 0,
            "p99_response_time": self._percentile(durations, 99) if durations else 0,
            "requests_per_second": len(self.results) / (max(r["timestamp"] for r in self.results) - min(r["timestamp"] for r in self.results)) if self.results else 0
        }
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """Î∞±Î∂ÑÏúÑÏàò Í≥ÑÏÇ∞"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
```

### 2. ÏÑ±Îä• ÌîÑÎ°úÌååÏùºÎßÅ

```python
import cProfile
import pstats
from io import StringIO
import memory_profiler
import line_profiler

class PerformanceProfiler:
    """ÏÑ±Îä• ÌîÑÎ°úÌååÏùºÎü¨"""
    
    @staticmethod
    def profile_function(func, *args, **kwargs):
        """Ìï®Ïàò ÌîÑÎ°úÌååÏùºÎßÅ"""
        profiler = cProfile.Profile()
        profiler.enable()
        
        result = func(*args, **kwargs)
        
        profiler.disable()
        
        # Í≤∞Í≥º Ï∂úÎ†•
        s = StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats()
        
        print("Function Profile:")
        print(s.getvalue())
        
        return result
    
    @staticmethod
    def memory_profile_function(func, *args, **kwargs):
        """Î©îÎ™®Î¶¨ ÌîÑÎ°úÌååÏùºÎßÅ"""
        @memory_profiler.profile
        def wrapper():
            return func(*args, **kwargs)
        
        return wrapper()
    
    @staticmethod
    def line_profile_function(func, *args, **kwargs):
        """ÎùºÏù∏ ÌîÑÎ°úÌååÏùºÎßÅ"""
        profiler = line_profiler.LineProfiler()
        profiler.add_function(func)
        profiler.enable()
        
        result = func(*args, **kwargs)
        
        profiler.disable()
        profiler.print_stats()
        
        return result
```

## üîß ÏÑ±Îä• ÏµúÏ†ÅÌôî Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏

### 1. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏµúÏ†ÅÌôî

- [ ] Ïù∏Îç±Ïä§ ÏµúÏ†ÅÌôî
- [ ] ÏøºÎ¶¨ ÏµúÏ†ÅÌôî
- [ ] Ïó∞Í≤∞ ÌíÄ ÏÑ§Ï†ï
- [ ] ÏøºÎ¶¨ Í≤∞Í≥º Ï∫êÏã±
- [ ] Î∞∞Ïπò Ï≤òÎ¶¨ Íµ¨ÌòÑ

### 2. Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏµúÏ†ÅÌôî

- [ ] ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ Íµ¨ÌòÑ
- [ ] Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÏµúÏ†ÅÌôî
- [ ] Ï∫êÏã± Ï†ÑÎûµ Íµ¨ÌòÑ
- [ ] ÏùëÎãµ ÏïïÏ∂ï ÏÑ§Ï†ï
- [ ] ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò Íµ¨ÌòÑ

### 3. Ïù∏ÌîÑÎùº ÏµúÏ†ÅÌôî

- [ ] Î°úÎìú Î∞∏Îü∞Ïã± ÏÑ§Ï†ï
- [ ] CDN ÏÇ¨Ïö©
- [ ] Ï∫êÏãú ÏÑúÎ≤Ñ Íµ¨ÏÑ±
- [ ] Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏä§ÌÖú Íµ¨Ï∂ï
- [ ] ÏûêÎèô Ïä§ÏºÄÏùºÎßÅ ÏÑ§Ï†ï

### 4. Î™®ÎãàÌÑ∞ÎßÅ Î∞è ÏïåÎ¶º

- [ ] ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏàòÏßë
- [ ] ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï
- [ ] ÏïåÎ¶º ÏãúÏä§ÌÖú Íµ¨Ï∂ï
- [ ] ÎåÄÏãúÎ≥¥Îìú Íµ¨ÏÑ±
- [ ] Î°úÍ∑∏ Î∂ÑÏÑù

Ïù¥ ÏÑ±Îä• ÌäúÎãù Í∞ÄÏù¥ÎìúÎ•º Îî∞Îùº Bridge ÏãúÏä§ÌÖúÏùò ÏÑ±Îä•ÏùÑ ÏµúÏ†ÅÌôîÌï† Ïàò ÏûàÏäµÎãàÎã§. ÏÑ±Îä• ÏµúÏ†ÅÌôîÎäî ÏßÄÏÜçÏ†ÅÏù∏ ÌîÑÎ°úÏÑ∏Ïä§Ïù¥ÎØÄÎ°ú Ï†ïÍ∏∞Ï†ÅÏúºÎ°ú Î™®ÎãàÌÑ∞ÎßÅÌïòÍ≥† Í∞úÏÑ†Ìï¥Ïïº Ìï©ÎãàÎã§.
