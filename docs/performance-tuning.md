# Bridge ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ

## ğŸ“– ê°œìš”

ì´ ë¬¸ì„œëŠ” Bridge ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ ê°€ì´ë“œì…ë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”, ìºì‹± ì „ëµ, ë¹„ë™ê¸° ì²˜ë¦¬, ë©”ëª¨ë¦¬ ê´€ë¦¬, ê·¸ë¦¬ê³  ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ì„±ëŠ¥ ê°œì„  ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ì„±ëŠ¥ ì¸¡ì • ë° ëª¨ë‹ˆí„°ë§

#### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì •ì˜

```python
from prometheus_client import Counter, Histogram, Gauge, Summary
import time
from functools import wraps

# ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter('bridge_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('bridge_request_duration_seconds', 'Request duration', ['method', 'endpoint'])
ACTIVE_CONNECTIONS = Gauge('bridge_active_connections', 'Active connections')
DATABASE_QUERY_DURATION = Histogram('bridge_db_query_duration_seconds', 'Database query duration', ['query_type'])
CACHE_HIT_RATIO = Gauge('bridge_cache_hit_ratio', 'Cache hit ratio')
MEMORY_USAGE = Gauge('bridge_memory_usage_bytes', 'Memory usage in bytes')
CPU_USAGE = Gauge('bridge_cpu_usage_percent', 'CPU usage percentage')

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.metrics = {}
        self.start_time = time.time()
    
    def track_request(self, method: str, endpoint: str, duration: float, status: str):
        """ìš”ì²­ ì¶”ì """
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
    
    def track_database_query(self, query_type: str, duration: float):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì¶”ì """
        DATABASE_QUERY_DURATION.labels(query_type=query_type).observe(duration)
    
    def update_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        import psutil
        process = psutil.Process()
        
        MEMORY_USAGE.set(process.memory_info().rss)
        CPU_USAGE.set(process.cpu_percent())
        ACTIVE_CONNECTIONS.set(len(process.connections()))
    
    def get_performance_summary(self) -> dict:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        uptime = time.time() - self.start_time
        return {
            "uptime_seconds": uptime,
            "memory_usage_mb": MEMORY_USAGE._value.get() / 1024 / 1024,
            "cpu_usage_percent": CPU_USAGE._value.get(),
            "active_connections": ACTIVE_CONNECTIONS._value.get()
        }

# ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°
def measure_performance(metric_name: str):
    """ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start_time
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                return result
            except Exception as e:
                duration = time.time() - start_time
                # ì—ëŸ¬ ë©”íŠ¸ë¦­ ê¸°ë¡
                raise
        return wrapper
    return decorator
```

#### ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ

```python
from fastapi import FastAPI, WebSocket
from typing import Dict, Any
import asyncio
import json

class PerformanceDashboard:
    """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self):
        self.connections = set()
        self.monitor = PerformanceMonitor()
    
    async def websocket_endpoint(self, websocket: WebSocket):
        """WebSocket ì—”ë“œí¬ì¸íŠ¸"""
        await websocket.accept()
        self.connections.add(websocket)
        
        try:
            while True:
                # ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
                performance_data = await self._collect_performance_data()
                
                # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ì†¡
                await websocket.send_text(json.dumps(performance_data))
                await asyncio.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                
        except Exception as e:
            print(f"WebSocket error: {e}")
        finally:
            self.connections.discard(websocket)
    
    async def _collect_performance_data(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘"""
        self.monitor.update_system_metrics()
        
        return {
            "timestamp": time.time(),
            "system": {
                "memory_usage_mb": MEMORY_USAGE._value.get() / 1024 / 1024,
                "cpu_usage_percent": CPU_USAGE._value.get(),
                "active_connections": ACTIVE_CONNECTIONS._value.get()
            },
            "requests": {
                "total_requests": REQUEST_COUNT._value.sum(),
                "avg_duration": REQUEST_DURATION._value.sum() / max(REQUEST_COUNT._value.sum(), 1)
            },
            "database": {
                "avg_query_duration": DATABASE_QUERY_DURATION._value.sum() / max(DATABASE_QUERY_DURATION._value.count(), 1)
            }
        }
```

### 2. ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”

#### ì¿¼ë¦¬ ìµœì í™”

```python
from sqlalchemy import text, select, func
from sqlalchemy.orm import sessionmaker
from typing import List, Dict, Any
import time

class QueryOptimizer:
    """ì¿¼ë¦¬ ìµœì í™”ê¸°"""
    
    def __init__(self, session_factory):
        self.session_factory = session_factory
        self.query_cache = {}
        self.query_stats = {}
    
    async def execute_optimized_query(self, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ìµœì í™”ëœ ì¿¼ë¦¬ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            # ì¿¼ë¦¬ ë¶„ì„
            query_plan = await self._analyze_query(query)
            
            # ì¸ë±ìŠ¤ íŒíŠ¸ ì¶”ê°€
            optimized_query = await self._add_index_hints(query, query_plan)
            
            # ì‹¤í–‰ ê³„íš í™•ì¸
            execution_plan = await self._get_execution_plan(optimized_query)
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            with self.session_factory() as session:
                result = session.execute(text(optimized_query), params or {})
                rows = [dict(row) for row in result]
            
            duration = time.time() - start_time
            
            # ì„±ëŠ¥ í†µê³„ ê¸°ë¡
            self._record_query_stats(query, duration, len(rows))
            
            return rows
            
        except Exception as e:
            duration = time.time() - start_time
            self._record_query_stats(query, duration, 0, error=str(e))
            raise
    
    async def _analyze_query(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ë¶„ì„"""
        # ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„
        complexity_score = self._calculate_complexity(query)
        
        # í…Œì´ë¸” ë° ì»¬ëŸ¼ ë¶„ì„
        tables = self._extract_tables(query)
        columns = self._extract_columns(query)
        
        # ì¡°ì¸ ë¶„ì„
        joins = self._analyze_joins(query)
        
        return {
            "complexity_score": complexity_score,
            "tables": tables,
            "columns": columns,
            "joins": joins
        }
    
    def _calculate_complexity(self, query: str) -> int:
        """ì¿¼ë¦¬ ë³µì¡ë„ ê³„ì‚°"""
        complexity = 0
        
        # SELECT ì ˆ ë³µì¡ë„
        complexity += query.upper().count('SELECT') * 1
        
        # JOIN ë³µì¡ë„
        complexity += query.upper().count('JOIN') * 2
        
        # ì„œë¸Œì¿¼ë¦¬ ë³µì¡ë„
        complexity += query.upper().count('(SELECT') * 3
        
        # ì§‘ê³„ í•¨ìˆ˜ ë³µì¡ë„
        aggregate_functions = ['COUNT', 'SUM', 'AVG', 'MAX', 'MIN', 'GROUP_CONCAT']
        for func in aggregate_functions:
            complexity += query.upper().count(func) * 1
        
        return complexity
    
    async def _add_index_hints(self, query: str, query_plan: Dict[str, Any]) -> str:
        """ì¸ë±ìŠ¤ íŒíŠ¸ ì¶”ê°€"""
        # í…Œì´ë¸”ë³„ ì¸ë±ìŠ¤ íŒíŠ¸ ì¶”ê°€
        for table in query_plan.get("tables", []):
            # ê°€ì¥ ì í•©í•œ ì¸ë±ìŠ¤ ì„ íƒ
            best_index = await self._select_best_index(table, query_plan)
            if best_index:
                query = query.replace(f"FROM {table}", f"FROM {table} USE INDEX ({best_index})")
        
        return query
    
    async def _select_best_index(self, table: str, query_plan: Dict[str, Any]) -> str:
        """ìµœì  ì¸ë±ìŠ¤ ì„ íƒ"""
        # í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
        indexes = await self._get_table_indexes(table)
        
        # ì¿¼ë¦¬ ì¡°ê±´ê³¼ ë§¤ì¹­ë˜ëŠ” ì¸ë±ìŠ¤ ì„ íƒ
        for index in indexes:
            if self._is_index_suitable(index, query_plan):
                return index["name"]
        
        return None
    
    async def _get_table_indexes(self, table: str) -> List[Dict[str, Any]]:
        """í…Œì´ë¸” ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
        return []
    
    def _is_index_suitable(self, index: Dict[str, Any], query_plan: Dict[str, Any]) -> bool:
        """ì¸ë±ìŠ¤ ì í•©ì„± ê²€ì‚¬"""
        # ì¸ë±ìŠ¤ ì»¬ëŸ¼ê³¼ ì¿¼ë¦¬ ì¡°ê±´ ë§¤ì¹­
        return True
    
    async def _get_execution_plan(self, query: str) -> Dict[str, Any]:
        """ì‹¤í–‰ ê³„íš ì¡°íšŒ"""
        # EXPLAIN ì¿¼ë¦¬ ì‹¤í–‰
        with self.session_factory() as session:
            result = session.execute(text(f"EXPLAIN {query}"))
            return [dict(row) for row in result]
    
    def _record_query_stats(self, query: str, duration: float, row_count: int, error: str = None):
        """ì¿¼ë¦¬ í†µê³„ ê¸°ë¡"""
        query_hash = hash(query)
        
        if query_hash not in self.query_stats:
            self.query_stats[query_hash] = {
                "query": query,
                "execution_count": 0,
                "total_duration": 0,
                "total_rows": 0,
                "error_count": 0
            }
        
        stats = self.query_stats[query_hash]
        stats["execution_count"] += 1
        stats["total_duration"] += duration
        stats["total_rows"] += row_count
        
        if error:
            stats["error_count"] += 1
```

#### ì—°ê²° í’€ ìµœì í™”

```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool, StaticPool
from sqlalchemy.engine import Engine
import asyncio

class DatabaseConnectionManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self, database_url: str, max_connections: int = 20):
        self.database_url = database_url
        self.max_connections = max_connections
        self.engine = self._create_optimized_engine()
        self.session_factory = sessionmaker(bind=self.engine)
    
    def _create_optimized_engine(self) -> Engine:
        """ìµœì í™”ëœ ì—”ì§„ ìƒì„±"""
        engine = create_engine(
            self.database_url,
            poolclass=QueuePool,
            pool_size=10,  # ê¸°ë³¸ ì—°ê²° ìˆ˜
            max_overflow=20,  # ì¶”ê°€ ì—°ê²° ìˆ˜
            pool_pre_ping=True,  # ì—°ê²° ìƒíƒœ í™•ì¸
            pool_recycle=3600,  # 1ì‹œê°„ë§ˆë‹¤ ì—°ê²° ì¬ìƒì„±
            echo=False,  # SQL ë¡œê¹… ë¹„í™œì„±í™”
            connect_args={
                "connect_timeout": 10,
                "application_name": "bridge_analytics"
            }
        )
        return engine
    
    async def get_connection(self):
        """ì—°ê²° íšë“"""
        return self.engine.connect()
    
    async def execute_query(self, query: str, params: Dict[str, Any] = None):
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        async with self.get_connection() as conn:
            result = await conn.execute(text(query), params or {})
            return [dict(row) for row in result]
    
    def get_pool_status(self) -> Dict[str, Any]:
        """ì—°ê²° í’€ ìƒíƒœ ì¡°íšŒ"""
        pool = self.engine.pool
        return {
            "pool_size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid()
        }
```

### 3. ìºì‹± ì „ëµ

#### ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ

```python
import redis
import json
import pickle
from typing import Any, Optional, Union
from functools import wraps
import hashlib

class MultiLayerCache:
    """ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        self.redis_client = redis.from_url(redis_url)
        self.local_cache = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "local_hits": 0,
            "redis_hits": 0
        }
    
    def cache_key(self, func_name: str, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{func_name}:{args}:{kwargs}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # 1ë‹¨ê³„: ë¡œì»¬ ìºì‹œ í™•ì¸
        if key in self.local_cache:
            self.cache_stats["hits"] += 1
            self.cache_stats["local_hits"] += 1
            return self.local_cache[key]
        
        # 2ë‹¨ê³„: Redis ìºì‹œ í™•ì¸
        try:
            cached_data = self.redis_client.get(key)
            if cached_data:
                data = pickle.loads(cached_data)
                self.local_cache[key] = data  # ë¡œì»¬ ìºì‹œì— ì €ì¥
                self.cache_stats["hits"] += 1
                self.cache_stats["redis_hits"] += 1
                return data
        except Exception as e:
            print(f"Redis cache error: {e}")
        
        self.cache_stats["misses"] += 1
        return None
    
    def set(self, key: str, data: Any, ttl: int = 3600):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        # ë¡œì»¬ ìºì‹œ ì €ì¥
        self.local_cache[key] = data
        
        # Redis ìºì‹œ ì €ì¥
        try:
            serialized_data = pickle.dumps(data)
            self.redis_client.setex(key, ttl, serialized_data)
        except Exception as e:
            print(f"Redis cache set error: {e}")
    
    def invalidate(self, pattern: str):
        """íŒ¨í„´ì— ë§ëŠ” ìºì‹œ ë¬´íš¨í™”"""
        # ë¡œì»¬ ìºì‹œ ë¬´íš¨í™”
        keys_to_remove = [k for k in self.local_cache.keys() if pattern in k]
        for key in keys_to_remove:
            del self.local_cache[key]
        
        # Redis ìºì‹œ ë¬´íš¨í™”
        try:
            keys = self.redis_client.keys(f"*{pattern}*")
            if keys:
                self.redis_client.delete(*keys)
        except Exception as e:
            print(f"Redis cache invalidation error: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_ratio = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            **self.cache_stats,
            "hit_ratio": hit_ratio,
            "local_cache_size": len(self.local_cache)
        }

# ìºì‹± ë°ì½”ë ˆì´í„°
def cached(ttl: int = 3600, cache_key_func: callable = None):
    """ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = MultiLayerCache()
            
            # ìºì‹œ í‚¤ ìƒì„±
            if cache_key_func:
                key = cache_key_func(*args, **kwargs)
            else:
                key = cache.cache_key(func.__name__, *args, **kwargs)
            
            # ìºì‹œì—ì„œ ì¡°íšŒ
            cached_result = cache.get(key)
            if cached_result is not None:
                return cached_result
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = await func(*args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            cache.set(key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

#### ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±

```python
class QueryResultCache:
    """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±"""
    
    def __init__(self, cache_manager: MultiLayerCache):
        self.cache_manager = cache_manager
        self.query_cache_ttl = {
            "metadata": 3600,  # 1ì‹œê°„
            "statistics": 1800,  # 30ë¶„
            "quality_check": 900,  # 15ë¶„
            "chart_data": 600,  # 10ë¶„
        }
    
    def cache_query_result(self, query_type: str, query: str, params: Dict[str, Any], result: Any):
        """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±"""
        cache_key = self._generate_cache_key(query_type, query, params)
        ttl = self.query_cache_ttl.get(query_type, 3600)
        self.cache_manager.set(cache_key, result, ttl)
    
    def get_cached_result(self, query_type: str, query: str, params: Dict[str, Any]) -> Optional[Any]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        cache_key = self._generate_cache_key(query_type, query, params)
        return self.cache_manager.get(cache_key)
    
    def _generate_cache_key(self, query_type: str, query: str, params: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{query_type}:{query}:{params}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def invalidate_query_cache(self, query_type: str = None):
        """ì¿¼ë¦¬ ìºì‹œ ë¬´íš¨í™”"""
        if query_type:
            self.cache_manager.invalidate(f"{query_type}:")
        else:
            self.cache_manager.invalidate("")
```

### 4. ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”

#### ë¹„ë™ê¸° ì‘ì—… í

```python
from celery import Celery
from celery.result import AsyncResult
from typing import Dict, Any, List
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncTaskManager:
    """ë¹„ë™ê¸° ì‘ì—… ê´€ë¦¬ì"""
    
    def __init__(self, celery_app: Celery, max_workers: int = 4):
        self.celery_app = celery_app
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_tasks = {}
    
    async def submit_task(self, task_name: str, args: tuple = (), kwargs: dict = None) -> str:
        """ì‘ì—… ì œì¶œ"""
        task = self.celery_app.send_task(task_name, args=args, kwargs=kwargs or {})
        self.active_tasks[task.id] = {
            "task": task,
            "status": "PENDING",
            "created_at": time.time()
        }
        return task.id
    
    async def get_task_result(self, task_id: str) -> Dict[str, Any]:
        """ì‘ì—… ê²°ê³¼ ì¡°íšŒ"""
        if task_id not in self.active_tasks:
            return {"error": "Task not found"}
        
        task_info = self.active_tasks[task_id]
        task = task_info["task"]
        
        if task.ready():
            if task.successful():
                result = task.result
                task_info["status"] = "SUCCESS"
                return {
                    "status": "SUCCESS",
                    "result": result
                }
            else:
                error = str(task.result)
                task_info["status"] = "FAILURE"
                return {
                    "status": "FAILURE",
                    "error": error
                }
        else:
            return {
                "status": "PENDING",
                "progress": task_info.get("progress", 0)
            }
    
    async def execute_parallel_tasks(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ ì‘ì—… ì‹¤í–‰"""
        async def execute_single_task(task_config):
            task_name = task_config["name"]
            args = task_config.get("args", ())
            kwargs = task_config.get("kwargs", {})
            
            return await self.submit_task(task_name, args, kwargs)
        
        # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì œì¶œ
        task_ids = await asyncio.gather(*[execute_single_task(task) for task in tasks])
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        for task_id in task_ids:
            result = await self.get_task_result(task_id)
            results.append({"task_id": task_id, **result})
        
        return results
    
    async def batch_process(self, items: List[Any], process_func: callable, 
                          batch_size: int = 100) -> List[Any]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            
            # ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            batch_tasks = [
                asyncio.create_task(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor, process_func, item
                    )
                ) for item in batch
            ]
            
            batch_results = await asyncio.gather(*batch_tasks)
            results.extend(batch_results)
        
        return results
```

#### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬

```python
import pandas as pd
import pyarrow as pa
from typing import Iterator, Any
import gc

class MemoryEfficientProcessor:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
    
    def process_large_dataset(self, data_source: str, process_func: callable) -> Iterator[Any]:
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì½ê¸°
        for chunk in self._read_data_in_chunks(data_source):
            try:
                # ì²­í¬ ì²˜ë¦¬
                processed_chunk = process_func(chunk)
                yield processed_chunk
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk
                gc.collect()
                
            except Exception as e:
                print(f"Chunk processing error: {e}")
                continue
    
    def _read_data_in_chunks(self, data_source: str) -> Iterator[pd.DataFrame]:
        """ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì½ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œë‚˜ íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©
        offset = 0
        
        while True:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì²­í¬ ì½ê¸°
            chunk = self._read_chunk_from_db(data_source, offset, self.chunk_size)
            
            if chunk.empty:
                break
            
            yield chunk
            offset += self.chunk_size
    
    def _read_chunk_from_db(self, data_source: str, offset: int, limit: int) -> pd.DataFrame:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì²­í¬ ì½ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰
        return pd.DataFrame()
    
    def optimize_memory_usage(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        # ë°ì´í„° íƒ€ì… ìµœì í™”
        for col in df.columns:
            if df[col].dtype == 'object':
                # ë¬¸ìì—´ ì»¬ëŸ¼ ìµœì í™”
                df[col] = df[col].astype('category')
            elif df[col].dtype == 'int64':
                # ì •ìˆ˜ ì»¬ëŸ¼ ìµœì í™”
                if df[col].min() >= 0:
                    if df[col].max() < 255:
                        df[col] = df[col].astype('uint8')
                    elif df[col].max() < 65535:
                        df[col] = df[col].astype('uint16')
                    elif df[col].max() < 4294967295:
                        df[col] = df[col].astype('uint32')
                else:
                    if df[col].min() > -128 and df[col].max() < 127:
                        df[col] = df[col].astype('int8')
                    elif df[col].min() > -32768 and df[col].max() < 32767:
                        df[col] = df[col].astype('int16')
                    elif df[col].min() > -2147483648 and df[col].max() < 2147483647:
                        df[col] = df[col].astype('int32')
            elif df[col].dtype == 'float64':
                # ì‹¤ìˆ˜ ì»¬ëŸ¼ ìµœì í™”
                df[col] = pd.to_numeric(df[col], downcast='float')
        
        return df
```

### 5. API ì„±ëŠ¥ ìµœì í™”

#### ì‘ë‹µ ì••ì¶•

```python
from fastapi import FastAPI, Request, Response
from fastapi.middleware.gzip import GZipMiddleware
import gzip
import json

def setup_response_compression(app: FastAPI):
    """ì‘ë‹µ ì••ì¶• ì„¤ì •"""
    app.add_middleware(GZipMiddleware, minimum_size=1000)
    
    @app.middleware("http")
    async def compress_response(request: Request, call_next):
        response = await call_next(request)
        
        # JSON ì‘ë‹µ ì••ì¶•
        if (response.headers.get("content-type", "").startswith("application/json") and
            len(response.body) > 1000):
            
            compressed_body = gzip.compress(response.body)
            response.body = compressed_body
            response.headers["content-encoding"] = "gzip"
            response.headers["content-length"] = str(len(compressed_body))
        
        return response
```

#### í˜ì´ì§€ë„¤ì´ì…˜ ìµœì í™”

```python
from typing import List, Dict, Any, Optional
from sqlalchemy import select, func

class PaginatedQuery:
    """í˜ì´ì§€ë„¤ì´ì…˜ ì¿¼ë¦¬"""
    
    def __init__(self, query, page: int = 1, page_size: int = 20):
        self.query = query
        self.page = max(1, page)
        self.page_size = min(100, max(1, page_size))  # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
        self.offset = (self.page - 1) * self.page_size
    
    async def execute(self, session) -> Dict[str, Any]:
        """í˜ì´ì§€ë„¤ì´ì…˜ ì¿¼ë¦¬ ì‹¤í–‰"""
        # ì´ ê°œìˆ˜ ì¡°íšŒ
        count_query = select(func.count()).select_from(self.query.subquery())
        total_count = session.execute(count_query).scalar()
        
        # í˜ì´ì§€ ë°ì´í„° ì¡°íšŒ
        paginated_query = self.query.offset(self.offset).limit(self.page_size)
        results = session.execute(paginated_query).fetchall()
        
        # í˜ì´ì§€ ì •ë³´ ê³„ì‚°
        total_pages = (total_count + self.page_size - 1) // self.page_size
        has_next = self.page < total_pages
        has_prev = self.page > 1
        
        return {
            "data": [dict(row) for row in results],
            "pagination": {
                "page": self.page,
                "page_size": self.page_size,
                "total_count": total_count,
                "total_pages": total_pages,
                "has_next": has_next,
                "has_prev": has_prev
            }
        }
```

#### API ì‘ë‹µ ìºì‹±

```python
from fastapi import FastAPI, Request, Response
from typing import Callable
import hashlib
import json

class APICacheMiddleware:
    """API ìºì‹± ë¯¸ë“¤ì›¨ì–´"""
    
    def __init__(self, cache_manager: MultiLayerCache):
        self.cache_manager = cache_manager
        self.cacheable_endpoints = {
            "/api/v1/analytics/statistics": 1800,  # 30ë¶„
            "/api/v1/connectors/metadata": 3600,   # 1ì‹œê°„
            "/api/v1/dashboards": 600,            # 10ë¶„
        }
    
    def __call__(self, request: Request, call_next: Callable) -> Response:
        # ìºì‹œ ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
        if request.url.path not in self.cacheable_endpoints:
            return call_next(request)
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(request)
        
        # ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ
        cached_response = self.cache_manager.get(cache_key)
        if cached_response:
            return Response(
                content=cached_response["content"],
                status_code=cached_response["status_code"],
                headers=cached_response["headers"]
            )
        
        # ì›ë³¸ ìš”ì²­ ì²˜ë¦¬
        response = call_next(request)
        
        # ì‘ë‹µ ìºì‹±
        if response.status_code == 200:
            ttl = self.cacheable_endpoints[request.url.path]
            self.cache_manager.set(cache_key, {
                "content": response.body,
                "status_code": response.status_code,
                "headers": dict(response.headers)
            }, ttl)
        
        return response
    
    def _generate_cache_key(self, request: Request) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{request.url.path}:{request.query_params}:{request.headers.get('authorization', '')}"
        return hashlib.md5(key_data.encode()).hexdigest()
```

### 6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

#### ì„±ëŠ¥ ì„ê³„ê°’ ëª¨ë‹ˆí„°ë§

```python
from typing import Dict, Any, List
import asyncio
import smtplib
from email.mime.text import MIMEText

class PerformanceAlertManager:
    """ì„±ëŠ¥ ì•Œë¦¼ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.thresholds = {
            "response_time": 2.0,  # 2ì´ˆ
            "memory_usage": 80.0,  # 80%
            "cpu_usage": 90.0,     # 90%
            "error_rate": 5.0,     # 5%
            "cache_hit_ratio": 0.7  # 70%
        }
        self.alert_history = []
        self.notification_channels = []
    
    def add_notification_channel(self, channel_type: str, config: Dict[str, Any]):
        """ì•Œë¦¼ ì±„ë„ ì¶”ê°€"""
        self.notification_channels.append({
            "type": channel_type,
            "config": config
        })
    
    async def check_performance_metrics(self, metrics: Dict[str, Any]):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸"""
        alerts = []
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        if metrics.get("avg_response_time", 0) > self.thresholds["response_time"]:
            alerts.append({
                "type": "high_response_time",
                "message": f"í‰ê·  ì‘ë‹µ ì‹œê°„ì´ {metrics['avg_response_time']:.2f}ì´ˆë¡œ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
                "severity": "warning"
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_usage = metrics.get("memory_usage_percent", 0)
        if memory_usage > self.thresholds["memory_usage"]:
            alerts.append({
                "type": "high_memory_usage",
                "message": f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {memory_usage:.1f}%ë¡œ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
                "severity": "critical" if memory_usage > 95 else "warning"
            })
        
        # CPU ì‚¬ìš©ëŸ‰ í™•ì¸
        cpu_usage = metrics.get("cpu_usage_percent", 0)
        if cpu_usage > self.thresholds["cpu_usage"]:
            alerts.append({
                "type": "high_cpu_usage",
                "message": f"CPU ì‚¬ìš©ëŸ‰ì´ {cpu_usage:.1f}%ë¡œ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
                "severity": "critical" if cpu_usage > 95 else "warning"
            })
        
        # ì—ëŸ¬ìœ¨ í™•ì¸
        error_rate = metrics.get("error_rate", 0)
        if error_rate > self.thresholds["error_rate"]:
            alerts.append({
                "type": "high_error_rate",
                "message": f"ì—ëŸ¬ìœ¨ì´ {error_rate:.1f}%ë¡œ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
                "severity": "critical"
            })
        
        # ìºì‹œ íˆíŠ¸ìœ¨ í™•ì¸
        cache_hit_ratio = metrics.get("cache_hit_ratio", 1.0)
        if cache_hit_ratio < self.thresholds["cache_hit_ratio"]:
            alerts.append({
                "type": "low_cache_hit_ratio",
                "message": f"ìºì‹œ íˆíŠ¸ìœ¨ì´ {cache_hit_ratio:.1%}ë¡œ ì„ê³„ê°’ ë¯¸ë§Œì…ë‹ˆë‹¤",
                "severity": "warning"
            })
        
        # ì•Œë¦¼ ì „ì†¡
        for alert in alerts:
            await self._send_alert(alert)
    
    async def _send_alert(self, alert: Dict[str, Any]):
        """ì•Œë¦¼ ì „ì†¡"""
        for channel in self.notification_channels:
            try:
                if channel["type"] == "email":
                    await self._send_email_alert(alert, channel["config"])
                elif channel["type"] == "slack":
                    await self._send_slack_alert(alert, channel["config"])
            except Exception as e:
                print(f"Alert sending failed: {e}")
    
    async def _send_email_alert(self, alert: Dict[str, Any], config: Dict[str, Any]):
        """ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""
        msg = MIMEText(alert["message"])
        msg["Subject"] = f"Bridge Performance Alert: {alert['type']}"
        msg["From"] = config["from_email"]
        msg["To"] = config["to_email"]
        
        # SMTP ì„œë²„ë¥¼ í†µí•œ ì´ë©”ì¼ ì „ì†¡
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¹„ë™ê¸° SMTP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        pass
    
    async def _send_slack_alert(self, alert: Dict[str, Any], config: Dict[str, Any]):
        """ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡"""
        # ìŠ¬ë™ ì›¹í›…ì„ í†µí•œ ì•Œë¦¼ ì „ì†¡
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” aiohttpë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ìš”ì²­
        pass
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

### 1. ë¶€í•˜ í…ŒìŠ¤íŠ¸

```python
import asyncio
import aiohttp
import time
from typing import List, Dict, Any
import statistics

class LoadTester:
    """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.results = []
    
    async def run_load_test(self, endpoint: str, concurrent_users: int, 
                           duration_seconds: int, request_data: Dict[str, Any] = None):
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(concurrent_users):
                task = asyncio.create_task(
                    self._simulate_user(session, endpoint, request_data, end_time)
                )
                tasks.append(task)
            
            await asyncio.gather(*tasks)
        
        return self._analyze_results()
    
    async def _simulate_user(self, session: aiohttp.ClientSession, endpoint: str, 
                           request_data: Dict[str, Any], end_time: float):
        """ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜"""
        while time.time() < end_time:
            start_time = time.time()
            
            try:
                if request_data:
                    async with session.post(f"{self.base_url}{endpoint}", 
                                          json=request_data) as response:
                        await response.text()
                else:
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        await response.text()
                
                duration = time.time() - start_time
                self.results.append({
                    "timestamp": start_time,
                    "duration": duration,
                    "status_code": response.status,
                    "success": response.status < 400
                })
                
            except Exception as e:
                duration = time.time() - start_time
                self.results.append({
                    "timestamp": start_time,
                    "duration": duration,
                    "status_code": 0,
                    "success": False,
                    "error": str(e)
                })
            
            # ìš”ì²­ ê°„ ê°„ê²©
            await asyncio.sleep(0.1)
    
    def _analyze_results(self) -> Dict[str, Any]:
        """ê²°ê³¼ ë¶„ì„"""
        if not self.results:
            return {}
        
        successful_requests = [r for r in self.results if r["success"]]
        failed_requests = [r for r in self.results if not r["success"]]
        
        durations = [r["duration"] for r in successful_requests]
        
        return {
            "total_requests": len(self.results),
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "success_rate": len(successful_requests) / len(self.results) if self.results else 0,
            "avg_response_time": statistics.mean(durations) if durations else 0,
            "min_response_time": min(durations) if durations else 0,
            "max_response_time": max(durations) if durations else 0,
            "p95_response_time": self._percentile(durations, 95) if durations else 0,
            "p99_response_time": self._percentile(durations, 99) if durations else 0,
            "requests_per_second": len(self.results) / (max(r["timestamp"] for r in self.results) - min(r["timestamp"] for r in self.results)) if self.results else 0
        }
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
```

### 2. ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§

```python
import cProfile
import pstats
from io import StringIO
import memory_profiler
import line_profiler

class PerformanceProfiler:
    """ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬"""
    
    @staticmethod
    def profile_function(func, *args, **kwargs):
        """í•¨ìˆ˜ í”„ë¡œíŒŒì¼ë§"""
        profiler = cProfile.Profile()
        profiler.enable()
        
        result = func(*args, **kwargs)
        
        profiler.disable()
        
        # ê²°ê³¼ ì¶œë ¥
        s = StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats()
        
        print("Function Profile:")
        print(s.getvalue())
        
        return result
    
    @staticmethod
    def memory_profile_function(func, *args, **kwargs):
        """ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§"""
        @memory_profiler.profile
        def wrapper():
            return func(*args, **kwargs)
        
        return wrapper()
    
    @staticmethod
    def line_profile_function(func, *args, **kwargs):
        """ë¼ì¸ í”„ë¡œíŒŒì¼ë§"""
        profiler = line_profiler.LineProfiler()
        profiler.add_function(func)
        profiler.enable()
        
        result = func(*args, **kwargs)
        
        profiler.disable()
        profiler.print_stats()
        
        return result
```

## ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

- [ ] ì¸ë±ìŠ¤ ìµœì í™”
- [ ] ì¿¼ë¦¬ ìµœì í™”
- [ ] ì—°ê²° í’€ ì„¤ì •
- [ ] ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±
- [ ] ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ìµœì í™”

- [ ] ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬í˜„
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- [ ] ìºì‹± ì „ëµ êµ¬í˜„
- [ ] ì‘ë‹µ ì••ì¶• ì„¤ì •
- [ ] í˜ì´ì§€ë„¤ì´ì…˜ êµ¬í˜„

### 3. ì¸í”„ë¼ ìµœì í™”

- [ ] ë¡œë“œ ë°¸ëŸ°ì‹± ì„¤ì •
- [ ] CDN ì‚¬ìš©
- [ ] ìºì‹œ ì„œë²„ êµ¬ì„±
- [ ] ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •

### 4. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] ì„ê³„ê°’ ì„¤ì •
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- [ ] ë¡œê·¸ ë¶„ì„

ì´ ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œë¥¼ ë”°ë¼ Bridge ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ ìµœì í™”ëŠ” ì§€ì†ì ì¸ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ ì •ê¸°ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.
